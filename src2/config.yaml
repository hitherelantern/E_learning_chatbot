# src2/config.yaml

milvus:
  host: "localhost"
  port: 19530
  database: "Transcriptions"
  alias: "Transcriptions"

llm:
  chat_model:
    name: "gemini-2.0-flash"
    temperature: 0.7
  embedding_model: 
    name: "models/embedding-001"
    batch_size: 2
    delay: 30
  

chunking:
  method: "semantic"
  semantic:
    overlap: 1
    window: 5
  recursive:
    chunk_size: 500
    overlap: 100

collection:
  dim: 768
  index_params:
    index_type: "IVF_FLAT"
    metric_type: "COSINE"
    params:
      nlist: 4

retrieval:
  search_params:
    metric_type: "COSINE"
    params:
      nprobe: 10
  top_k: 10
  prompt_template: "template.json"

transcription:
  audio_folder: "temp_audio"
  transcriptions_folder: "temp_transcriptions"
  model_size: "base"
  device: "cpu"
  interval: 15

database:
  server: "mongodb://172.16.1.32:27027/"
  db : "DATA_SCIENCE"
  chat_history: "e_learning_bot_history"
  conversations: "e_learning_bot_conversations"
  session_summary: "e_learning_bot_summaries"



backend:
  server: "http://localhost:8001" 
